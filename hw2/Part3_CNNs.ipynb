{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 3: Convolutional Architectures\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will explore convolution networks and the effects of their architecture on accuracy. We'll implement a common block-based deep CNN pattern and we'll perform various experiments on it while varying the architecture. Then we'll implement our own custom architecture to see whether we can get high classification results on a large subset of CIFAR-10.\n",
    "\n",
    "Training will be performed on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as tvtf\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "test = unittest.TestCase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layers and networks\n",
    "<a id=part3_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers are the most essential building blocks of the state of the art deep learning image classification models and also play an important role in many other tasks.\n",
    "As we saw in the tutorial, when applied to images, convolutional layers operate on and produce volumes (3D tensors)\n",
    "of activations.\n",
    "\n",
    "One way to think about them is as if the neurons are organized in a 3D grid,\n",
    "where neurons at the same depth share weights (represented here as colors).\n",
    "Contrary to fully connected (affine) layers, neurons in convolutional layers are **not** connected to each of\n",
    "the activations of the previous layer.\n",
    "Instead, each neuron is connected only to a small region of the input volume,\n",
    "for example a 5x5x$C_{\\mathrm{in}}$ slice (where $C_{\\mathrm{in}}$ is the input volume's depth).\n",
    "\n",
    "<img src=\"imgs/depthcol.jpeg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to interpret convolutional layers for images is as a collection of 3D learnable filters,\n",
    "each of which operates on a small spatial region of the input volume.\n",
    "Each filter is convolved with the input volume (\"slides over it\"),\n",
    "and a dot product is computed at each location followed by a non-linearity which produces one activation.\n",
    "All these activations produce a 2D plane known as a **feature map**.\n",
    "Multiple feature maps (one for each filter) comprise the output volume.\n",
    "\n",
    "<img src=\"imgs/cnn_filters.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial property of convolutional layers is their translation equivariance, i.e. shifting the input results in\n",
    "and equivalently shifted output.\n",
    "This produces the ability to detect features regardless of their spatial location in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional network architectures usually follow a pattern basic repeating blocks: one or more convolution layers, each followed by a non-linearity (generally ReLU) and then a pooling layer to reduce spatial dimensions. Usually, the number of convolutional filters increases the deeper they are in the network.\n",
    "These layers are meant to extract features from the input.\n",
    "Then, one or more fully-connected layers is used to combine the extracted features into the required number of output class scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building convolutional networks with PyTorch\n",
    "<a id=part3_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides all the basic building blocks needed for creating a convolutional arcitecture within the [`torch.nn`](https://pytorch.org/docs/stable/nn.html) package.\n",
    "Let's use them to create a basic convolutional network with the following architecture pattern:\n",
    "\n",
    "    [(CONV -> ReLU)*P -> MaxPool]*(N/P) -> (Linear -> ReLU)*M -> Linear\n",
    "\n",
    "Here $N$ is the total number of convolutional layers,\n",
    "$P$ specifies how many convolutions to perform before each pooling layer\n",
    "and $M$ specifies the number of hidden fully-connected layers before the final output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementaion of the `ConvClassifier` class in the `hw2/cnn.py` module.\n",
    "Use PyTorch's `nn.Conv2d` and `nn.MaxPool2d` for the convolution and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvClassifier(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=20000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "out = tensor([[-0.0868, -0.3790, -0.4341, -0.1236, -0.2160,  0.1683,  0.4739,  0.0750,\n",
      "          0.1151, -0.1606]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import hw2.cnn as cnn\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "net = cnn.ConvClassifier((3,100,100), 10, channels=[32]*4, pool_every=2, hidden_dims=[100]*2)\n",
    "print(net)\n",
    "\n",
    "test_image = torch.randint(low=0, high=256, size=(3, 100, 100), dtype=torch.float).unsqueeze(0)\n",
    "test_out = net(test_image)\n",
    "print('out =', test_out)\n",
    "\n",
    "expected_out = torch.load('tests/assets/expected_conv_out.pt')\n",
    "test.assertLess(torch.norm(test_out - expected_out).item(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load CIFAR-10 again to use as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train: 50000 samples\n",
      "Test: 10000 samples\n",
      "input image size = torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "ds_train = torchvision.datasets.CIFAR10(root=data_dir, download=True, train=True, transform=tvtf.ToTensor())\n",
    "ds_test = torchvision.datasets.CIFAR10(root=data_dir, download=True, train=False, transform=tvtf.ToTensor())\n",
    "\n",
    "print(f'Train: {len(ds_train)} samples')\n",
    "print(f'Test: {len(ds_test)} samples')\n",
    "\n",
    "x0,_ = ds_train[0]\n",
    "in_size = x0.shape\n",
    "num_classes = 10\n",
    "print('input image size =', in_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as usual, as a sanity test let's make sure we can overfit a tiny dataset with our model. But first we need to adapt our `Trainer` for PyTorch models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementaion of the `TorchTrainer` class in the `hw2/training.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch (Avg. Loss 2.378, Accuracy 0.0): 100%|█████████████████████████████████████| 25/25 [00:01<00:00, 13.77it/s]\n",
      "train_batch (Avg. Loss 2.220, Accuracy 20.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 199.95it/s]\n",
      "train_batch (Avg. Loss 2.110, Accuracy 22.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 183.78it/s]\n",
      "train_batch (Avg. Loss 1.786, Accuracy 32.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 186.52it/s]\n",
      "train_batch (Avg. Loss 1.366, Accuracy 52.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 182.44it/s]\n",
      "train_batch (Avg. Loss 1.248, Accuracy 54.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 198.37it/s]\n",
      "train_batch (Avg. Loss 1.205, Accuracy 80.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 199.95it/s]\n",
      "train_batch (Avg. Loss 0.141, Accuracy 96.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 199.96it/s]\n",
      "train_batch (Avg. Loss 0.354, Accuracy 84.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 192.26it/s]\n",
      "train_batch (Avg. Loss 1.278, Accuracy 70.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 196.81it/s]\n",
      "train_batch (Avg. Loss 0.273, Accuracy 94.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 192.26it/s]\n",
      "train_batch (Avg. Loss 0.047, Accuracy 98.0): 100%|███████████████████████████████████| 25/25 [00:00<00:00, 196.81it/s]\n",
      "train_batch (Avg. Loss 0.004, Accuracy 100.0): 100%|██████████████████████████████████| 25/25 [00:00<00:00, 196.81it/s]\n",
      "train_batch (Avg. Loss 0.001, Accuracy 100.0): 100%|██████████████████████████████████| 25/25 [00:00<00:00, 193.75it/s]\n",
      "train_batch (Avg. Loss 0.001, Accuracy 100.0): 100%|██████████████████████████████████| 25/25 [00:00<00:00, 187.93it/s]\n"
     ]
    }
   ],
   "source": [
    "import hw2.training as training\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Define a tiny part of the CIFAR-10 dataset to overfit it\n",
    "batch_size = 2\n",
    "max_batches = 25\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=False)\n",
    "\n",
    "# Create model, loss and optimizer instances\n",
    "model = cnn.ConvClassifier(in_size, num_classes, channels=[32], pool_every=1, hidden_dims=[100])\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9,)\n",
    "\n",
    "# Use TorchTrainer to run only the training loop a few times.\n",
    "trainer = training.TorchTrainer(model, loss_fn, optimizer, device)\n",
    "best_acc = 0\n",
    "for i in range(30):\n",
    "    res = trainer.train_epoch(dl_train, max_batches=max_batches, verbose=(i%2==0))\n",
    "    best_acc = res.accuracy if res.accuracy > best_acc else best_acc\n",
    "    \n",
    "# Test overfitting\n",
    "test.assertGreaterEqual(best_acc, 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common addition to the basic convolutional architecture described above are **shortcut connections**.\n",
    "First proposed by [He et al. (2016)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), this simple addition has been shown to be crucial\n",
    "ingredient in order to achieve effective learning with very deep networks.\n",
    "Virtually all state of the art image classification models from recent years use this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to add an shortcut, or skip, around every two or more convolutional layers:\n",
    "\n",
    "<img src=\"imgs/resnet_block2.png\" width=\"700\" />\n",
    "\n",
    "This adds an easy way for the network to learn identity mappings: set the weight values to be very small.\n",
    "The consequence is that the convolutional layers to learn a **residual** mapping, i.e. some delta that is applied\n",
    "to the identity map, instead of actually learning a completely new mapping from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by implementing a general residual block, representing a structure similar to the above diagrams.\n",
    "Our residual block will be composed of:\n",
    "- A \"main path\" with some number of convolutional layers with ReLU between them. Optionally, we'll also allow batch normalization and dropout layers between the convolutions, before the ReLU.\n",
    "- A \"shortcut path\" implementing an identity mapping around the main path. In case of a different number of input/output channels, the shortcut path may contain an additional `1x1` convolution to project the channel dimension.\n",
    "- The sum of the main and shortcut paths output is passed though a ReLU and returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `ResidualBlock`'s `__init__()` method in the `hw2/cnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualBlock(\n",
      "  (shortcut_path): Sequential(\n",
      "    (0): Conv2d(3, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (main_path): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Dropout2d(p=0.2, inplace=False)\n",
      "    (2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(6, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): Dropout2d(p=0.2, inplace=False)\n",
      "    (7): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Conv2d(4, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Dropout2d(p=0.2, inplace=False)\n",
      "    (11): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): Conv2d(6, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      ")\n",
      "out shape=torch.Size([1, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "resblock = cnn.ResidualBlock(\n",
    "    in_channels=3, channels=[6, 4]*2, kernel_sizes=[3, 5]*2, batchnorm=True, dropout=0.2\n",
    ")\n",
    "\n",
    "print(resblock)\n",
    "test_out = resblock(torch.zeros(1, 3, 32, 32))\n",
    "print(f'out shape={test_out.shape}')\n",
    "\n",
    "expected_out = torch.load('tests/assets/expected_resblock_out.pt')\n",
    "test.assertLess(torch.norm(test_out - expected_out).item(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now based on the `ResidualBlock`, we'll implement our own variation of a residual network (ResNet),\n",
    "with the following architecture:\n",
    "\n",
    "    [-> (CONV -> ReLU)*P -> MaxPool]*(N/P) -> (Linear -> ReLU)*M -> Linear\n",
    "     \\------- SKIP ------/\n",
    "     \n",
    "Note that $N$, $P$ and $M$ are as before, however now $P$ also controls the number of convolutional layers to add a skip-connection to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `ResNetClassifier` class in the `hw2/cnn.py` module.\n",
    "You should use `ResidualBlock`s to group together every $P$ convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetClassifier(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (shortcut_path): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (main_path): Sequential(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Dropout2d(p=0.0, inplace=False)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU()\n",
      "        (5): Dropout2d(p=0.0, inplace=False)\n",
      "        (6): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): ReLU()\n",
      "        (8): Dropout2d(p=0.0, inplace=False)\n",
      "        (9): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ResidualBlock(\n",
      "      (shortcut_path): Sequential()\n",
      "      (main_path): Sequential(\n",
      "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Dropout2d(p=0.0, inplace=False)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=160000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "out = tensor([[ 0.7512, -0.5055, 12.0261,  3.6577, -1.6581, 11.0309,  6.6823,  1.1269,\n",
      "          6.9933,  4.3001]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "net = cnn.ResNetClassifier(\n",
    "    in_size=(3,100,100), out_classes=10, channels=[32, 64]*3, pool_every=4, hidden_dims=[100]*2\n",
    ")\n",
    "print(net)\n",
    "\n",
    "test_image = torch.randint(low=0, high=256, size=(3, 100, 100), dtype=torch.float).unsqueeze(0)\n",
    "test_out = net(test_image)\n",
    "print('out =', test_out)\n",
    "\n",
    "expected_out = torch.load('tests/assets/expected_resnet_out.pt')\n",
    "test.assertLess(torch.norm(test_out - expected_out).item(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with model architectures\n",
    "<a id=part3_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now perform a series of experiments that train various model configurations on a much larger part of the CIFAR-10 dataset.\n",
    "\n",
    "To perform the experiments, you'll need to use a machine with a GPU since training time might be too long otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note about running on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of running a forward pass on the GPU\n",
    "(assuming you're running this notebook on a GPU-enabled machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "test_image = test_image.to(device)\n",
    "test_out = net(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we called `.to(device)` on **both** the model and the input tensor.\n",
    "Here the `device` is a `torch.device` object that we created above. If an nvidia GPU is available on the machine you're running this on, the `device` will be `'cuda'`. When you run `.to(device)` on a model, it recursively goes over all the model parameter tensors and copies their memory to the GPU. Similarly, calling `.to(device)` on the input image also copies it.\n",
    "\n",
    "In order to train on a GPU, you need to make sure to move **all** your tensors to it. You'll get errors if you try to mix CPU and GPU tensors in a computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is running with device=cuda\n",
      "The model parameter tensors are also on device=cuda:0\n",
      "The test image is also on device=cuda:0\n",
      "The output is therefore also on device=cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f'This notebook is running with device={device}')\n",
    "print(f'The model parameter tensors are also on device={next(net.parameters()).device}')\n",
    "print(f'The test image is also on device={test_image.device}')\n",
    "print(f'The output is therefore also on device={test_out.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General notes for running experiments\n",
    "\n",
    "- You can run the experiments on a different machine (e.g. the course servers) and copy the results (files)\n",
    "  to the `results` folder on your local machine.\n",
    "  This notebook will only display the results, not run the actual experiment code (except for a demo run).\n",
    "- It's important to give each experiment run a name as specified by the notebook instructions later on.\n",
    "  Each run has a `run_name` parameter that will also be the base name of the results file which this \n",
    "  notebook will expect to load.\n",
    "- You will implement the code to run the experiments in the `hw2/experiments.py` module.\n",
    "  This module has a CLI parser so that you can invoke it as a script and pass in all the\n",
    "  configuration parameters for a single experiment run.\n",
    "- You should use `python -m hw2.experiments run-exp` to run an experiment, and **not**\n",
    "  `python hw2/experiments.py run-exp`, regardless of how/where you run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Network depth and number of filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will test some different architecture configurations based on our `ConvClassifier` and `ResNetClassifier`.\n",
    "Specifically, we want to try different depths and number of features to see the effects these parameters have on the model's performance.\n",
    "\n",
    "To do this, we'll define two extra hyperparameters for our model, `K` (`filters_per_layer`) and `L` (`layers_per_block`).\n",
    "- `K` is a list, containing the number of filters we want to have in our conv layers.\n",
    "- `L` is the number of consecutive layers with the same number of filters to use.\n",
    "\n",
    "For example, if `K=[32, 64]` and `L=2` it means we want two conv layers with 32 filters followed by two conv layers with 64 filters. The feature-extraction part of our model will therefore be:\n",
    "\n",
    "    Conv(X,32)->ReLu->Conv(32,32)->ReLU->MaxPool->Conv(32,64)->ReLU->Conv(64,64)->ReLU->MaxPool\n",
    "    \n",
    "We'll try various values of the `K` and `L` parameters in combination and see how each architecture trains. **All other hyperparameters are up to you**, including the choice of the optimization algorithm, the learning rate, regularization and architecture hyperparams such as `pad_every` and `hidden_dims`. Note that you should select the `pad_every` parameter wisely per experiment so that you don't end up with zero-with feature maps.\n",
    "\n",
    "You can try some short manual runs to determine some good values for the hyperparameters or implement cross-validation to do it. However, the **dataset size** you test on should be large. Use at least ~20000 training images and ~6000 validation images.\n",
    "\n",
    "The important thing is that you state what you used, how you decided on it, and explain your results based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to write some code to run the experiment.\n",
    "\n",
    "**TODO**:\n",
    "1. Implement the `run_experiment()` function in the `hw2/experiments.py` module.\n",
    "1. If you haven't done so already, it would be an excellent idea to implement the **early stopping** feature of the `Trainer` class.\n",
    "\n",
    "The following block tests that your implementation works. It's also meant to show you that each experiment run creates a result file containing the parameters to reproduce and the `FitResult` object for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "--- EPOCH 1/10 ---\n",
      "train_batch (Avg. Loss 2.068, Accuracy 25.6): 100%|████████████████████████████████| 1000/1000 [00:13<00:00, 73.13it/s]\n",
      "Epoch 0 train loss: 2.067569091796875\n",
      "test_batch (Avg. Loss 1.885, Accuracy 33.3): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 201.84it/s]\n",
      "--- EPOCH 2/10 ---\n",
      "train_batch (Avg. Loss 1.739, Accuracy 38.1): 100%|████████████████████████████████| 1000/1000 [00:13<00:00, 73.30it/s]\n",
      "Epoch 1 train loss: 1.7390069580078125\n",
      "test_batch (Avg. Loss 1.606, Accuracy 42.9): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 199.33it/s]\n",
      "--- EPOCH 3/10 ---\n",
      "train_batch (Avg. Loss 1.552, Accuracy 44.5): 100%|████████████████████████████████| 1000/1000 [00:13<00:00, 73.21it/s]\n",
      "Epoch 2 train loss: 1.551714111328125\n",
      "test_batch (Avg. Loss 1.478, Accuracy 47.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 197.45it/s]\n",
      "--- EPOCH 4/10 ---\n",
      "train_batch (Avg. Loss 1.446, Accuracy 48.3): 100%|████████████████████████████████| 1000/1000 [00:13<00:00, 72.99it/s]\n",
      "Epoch 3 train loss: 1.445771484375\n",
      "test_batch (Avg. Loss 1.371, Accuracy 51.5): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 200.34it/s]\n",
      "--- EPOCH 5/10 ---\n",
      "train_batch (Avg. Loss 1.356, Accuracy 51.7): 100%|████████████████████████████████| 1000/1000 [00:13<00:00, 72.54it/s]\n",
      "Epoch 4 train loss: 1.355756591796875\n",
      "test_batch (Avg. Loss 1.289, Accuracy 54.4): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 200.68it/s]\n",
      "--- EPOCH 6/10 ---\n",
      "train_batch (Avg. Loss 1.284, Accuracy 54.6): 100%|████████████████████████████████| 1000/1000 [00:13<00:00, 72.82it/s]\n",
      "Epoch 5 train loss: 1.28365771484375\n",
      "test_batch (Avg. Loss 1.230, Accuracy 56.6): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 197.82it/s]\n",
      "--- EPOCH 7/10 ---\n",
      "train_batch (Avg. Loss 1.223, Accuracy 56.9): 100%|████████████████████████████████| 1000/1000 [00:13<00:00, 73.21it/s]\n",
      "Epoch 6 train loss: 1.223127685546875\n",
      "test_batch (Avg. Loss 1.185, Accuracy 58.3): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 197.12it/s]\n",
      "--- EPOCH 8/10 ---\n",
      "train_batch (Avg. Loss 1.169, Accuracy 59.0): 100%|████████████████████████████████| 1000/1000 [00:13<00:00, 73.13it/s]\n",
      "Epoch 7 train loss: 1.1691837158203124\n",
      "test_batch (Avg. Loss 1.145, Accuracy 59.3): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 198.76it/s]\n",
      "--- EPOCH 9/10 ---\n",
      "train_batch (Avg. Loss 1.119, Accuracy 60.9): 100%|████████████████████████████████| 1000/1000 [00:13<00:00, 72.74it/s]\n",
      "Epoch 8 train loss: 1.1194189453125\n",
      "test_batch (Avg. Loss 1.115, Accuracy 60.8): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 198.62it/s]\n",
      "--- EPOCH 10/10 ---\n",
      "train_batch (Avg. Loss 1.073, Accuracy 62.5): 100%|████████████████████████████████| 1000/1000 [00:13<00:00, 72.91it/s]\n",
      "Epoch 9 train loss: 1.0731580810546875\n",
      "test_batch (Avg. Loss 1.084, Accuracy 61.8): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 198.29it/s]\n",
      "*** Output file ./results\\test_run_L1_K32-64.json written\n",
      "experiment config:  {'run_name': 'test_run', 'out_dir': './results', 'seed': 42, 'device': None, 'bs_train': 50, 'bs_test': 12, 'batches': 10, 'epochs': 10, 'early_stopping': 5, 'checkpoints': None, 'lr': 0.001, 'reg': 0.001, 'filters_per_layer': [32, 64], 'pool_every': 1, 'hidden_dims': [100], 'model_type': 'resnet', 'kw': {}, 'layers_per_block': 1}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hw2.experiments as experiments\n",
    "from hw2.experiments import load_experiment\n",
    "from cs236781.plot import plot_fit\n",
    "\n",
    "# Test experiment1 implementation on a few data samples and with a small model\n",
    "\n",
    "experiments.run_experiment(\n",
    "    'test_run', seed=seed, bs_train=50, batches=10, epochs=10, early_stopping=5,\n",
    "    filters_per_layer=[32,64], layers_per_block=1, pool_every=1, hidden_dims=[100],\n",
    "    model_type='resnet',\n",
    ")\n",
    "\n",
    "# There should now be a file 'test_run.json' in your `results/` folder.\n",
    "# We can use it to load the results of the experiment.\n",
    "cfg, fit_res = load_experiment('results/test_run_L1_K32-64.json')\n",
    "_, _ = plot_fit(fit_res)\n",
    "\n",
    "# And `cfg` contains the exact parameters to reproduce it\n",
    "print('experiment config: ', cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to load multiple experiment results and plot them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_exp_results(filename_pattern, results_dir='results'):\n",
    "    fig = None\n",
    "    result_files = glob.glob(os.path.join(results_dir, filename_pattern))\n",
    "    result_files.sort()\n",
    "    if len(result_files) == 0:\n",
    "        print(f'No results found for pattern {filename_pattern}.', file=sys.stderr)\n",
    "        return\n",
    "    for filepath in result_files:\n",
    "        m = re.match('exp\\d_(\\d_)?(.*)\\.json', os.path.basename(filepath))\n",
    "        cfg, fit_res = load_experiment(filepath)\n",
    "        fig, axes = plot_fit(fit_res, fig, legend=m[2],log_loss=True)\n",
    "    del cfg['filters_per_layer']\n",
    "    del cfg['layers_per_block']\n",
    "    print('common config: ', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "--- EPOCH 1/35 ---\n",
      "train_batch (Avg. Loss 1.974, Accuracy 29.4): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 65.83it/s]\n",
      "Epoch 0 train loss: 1.9736385498046876\n",
      "test_batch (Avg. Loss 1.768, Accuracy 38.0): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 193.73it/s]\n",
      "--- EPOCH 2/35 ---\n",
      "train_batch (Avg. Loss 1.656, Accuracy 41.3): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 67.45it/s]\n",
      "Epoch 1 train loss: 1.655955322265625\n",
      "test_batch (Avg. Loss 1.536, Accuracy 45.3): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 192.74it/s]\n",
      "--- EPOCH 3/35 ---\n",
      "train_batch (Avg. Loss 1.489, Accuracy 46.8): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 67.11it/s]\n",
      "Epoch 2 train loss: 1.488790283203125\n",
      "test_batch (Avg. Loss 1.401, Accuracy 49.7): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 195.32it/s]\n",
      "--- EPOCH 4/35 ---\n",
      "train_batch (Avg. Loss 1.368, Accuracy 51.3): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 66.80it/s]\n",
      "Epoch 3 train loss: 1.3681248779296875\n",
      "test_batch (Avg. Loss 1.296, Accuracy 53.1): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 192.61it/s]\n",
      "--- EPOCH 5/35 ---\n",
      "train_batch (Avg. Loss 1.267, Accuracy 54.8): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 67.32it/s]\n",
      "Epoch 4 train loss: 1.267037109375\n",
      "test_batch (Avg. Loss 1.227, Accuracy 56.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 195.18it/s]\n",
      "--- EPOCH 6/35 ---\n",
      "train_batch (Avg. Loss 1.182, Accuracy 58.0): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 67.13it/s]\n",
      "Epoch 5 train loss: 1.182213134765625\n",
      "test_batch (Avg. Loss 1.164, Accuracy 58.7): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 196.28it/s]\n",
      "--- EPOCH 7/35 ---\n",
      "train_batch (Avg. Loss 1.100, Accuracy 61.1): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 67.12it/s]\n",
      "Epoch 6 train loss: 1.0999456787109374\n",
      "test_batch (Avg. Loss 1.101, Accuracy 61.1): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 195.78it/s]\n",
      "--- EPOCH 8/35 ---\n",
      "train_batch (Avg. Loss 1.026, Accuracy 63.9): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 66.74it/s]\n",
      "Epoch 7 train loss: 1.02595556640625\n",
      "test_batch (Avg. Loss 1.060, Accuracy 62.9): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 195.64it/s]\n",
      "--- EPOCH 9/35 ---\n",
      "train_batch (Avg. Loss 0.963, Accuracy 66.4): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 66.73it/s]\n",
      "Epoch 8 train loss: 0.9626812133789062\n",
      "test_batch (Avg. Loss 1.038, Accuracy 63.6): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 197.03it/s]\n",
      "--- EPOCH 10/35 ---\n",
      "train_batch (Avg. Loss 0.905, Accuracy 68.4): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 67.05it/s]\n",
      "Epoch 9 train loss: 0.9048724975585938\n",
      "test_batch (Avg. Loss 1.022, Accuracy 64.4): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 195.32it/s]\n",
      "--- EPOCH 11/35 ---\n",
      "train_batch (Avg. Loss 0.851, Accuracy 70.3): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 66.85it/s]\n",
      "Epoch 10 train loss: 0.8509041137695312\n",
      "test_batch (Avg. Loss 1.013, Accuracy 64.8): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 193.41it/s]\n",
      "--- EPOCH 12/35 ---\n",
      "train_batch (Avg. Loss 0.799, Accuracy 72.2): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 66.85it/s]\n",
      "Epoch 11 train loss: 0.7988964233398438\n",
      "test_batch (Avg. Loss 1.020, Accuracy 65.1): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 192.17it/s]\n",
      "--- EPOCH 13/35 ---\n",
      "train_batch (Avg. Loss 0.747, Accuracy 74.0): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 66.67it/s]\n",
      "Epoch 12 train loss: 0.7469227294921875\n",
      "test_batch (Avg. Loss 1.016, Accuracy 65.5): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 195.09it/s]\n",
      "--- EPOCH 14/35 ---\n",
      "train_batch (Avg. Loss 0.694, Accuracy 75.9): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 66.43it/s]\n",
      "Epoch 13 train loss: 0.694002197265625\n",
      "test_batch (Avg. Loss 1.002, Accuracy 66.5): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 193.91it/s]\n",
      "--- EPOCH 15/35 ---\n",
      "train_batch (Avg. Loss 0.642, Accuracy 77.9): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 65.92it/s]\n",
      "Epoch 14 train loss: 0.6423265991210938\n",
      "test_batch (Avg. Loss 1.001, Accuracy 67.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 193.28it/s]\n",
      "--- EPOCH 16/35 ---\n",
      "train_batch (Avg. Loss 0.589, Accuracy 79.9): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 66.58it/s]\n",
      "Epoch 15 train loss: 0.589270751953125\n",
      "test_batch (Avg. Loss 1.019, Accuracy 67.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 189.93it/s]\n",
      "--- EPOCH 17/35 ---\n",
      "train_batch (Avg. Loss 0.536, Accuracy 81.9): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 66.51it/s]\n",
      "Epoch 16 train loss: 0.5357281494140625\n",
      "test_batch (Avg. Loss 1.051, Accuracy 67.4): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 191.86it/s]\n",
      "--- EPOCH 18/35 ---\n",
      "train_batch (Avg. Loss 0.485, Accuracy 83.8): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 65.92it/s]\n",
      "Epoch 17 train loss: 0.4845277099609375\n",
      "test_batch (Avg. Loss 1.111, Accuracy 66.8): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 191.81it/s]\n",
      "--- EPOCH 19/35 ---\n",
      "train_batch (Avg. Loss 0.435, Accuracy 85.6): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 66.79it/s]\n",
      "Epoch 18 train loss: 0.4345858154296875\n",
      "test_batch (Avg. Loss 1.205, Accuracy 66.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 193.77it/s]\n",
      "--- EPOCH 20/35 ---\n",
      "train_batch (Avg. Loss 0.389, Accuracy 87.2): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 66.52it/s]\n",
      "Epoch 19 train loss: 0.38860733032226563\n",
      "test_batch (Avg. Loss 1.309, Accuracy 65.6): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 193.95it/s]\n",
      "*** Output file ./results\\test_run_L2_K32.json written\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "--- EPOCH 1/35 ---\n",
      "train_batch (Avg. Loss 1.921, Accuracy 31.4): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.80it/s]\n",
      "Epoch 0 train loss: 1.9212650146484376\n",
      "test_batch (Avg. Loss 1.702, Accuracy 39.9): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 190.89it/s]\n",
      "--- EPOCH 2/35 ---\n",
      "train_batch (Avg. Loss 1.587, Accuracy 43.4): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.72it/s]\n",
      "Epoch 1 train loss: 1.586666015625\n",
      "test_batch (Avg. Loss 1.484, Accuracy 46.8): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 186.45it/s]\n",
      "--- EPOCH 3/35 ---\n",
      "train_batch (Avg. Loss 1.421, Accuracy 49.3): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.63it/s]\n",
      "Epoch 2 train loss: 1.4209219970703124\n",
      "test_batch (Avg. Loss 1.342, Accuracy 51.8): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 191.33it/s]\n",
      "--- EPOCH 4/35 ---\n",
      "train_batch (Avg. Loss 1.282, Accuracy 54.4): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.61it/s]\n",
      "Epoch 3 train loss: 1.281658203125\n",
      "test_batch (Avg. Loss 1.221, Accuracy 56.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 189.46it/s]\n",
      "--- EPOCH 5/35 ---\n",
      "train_batch (Avg. Loss 1.163, Accuracy 58.9): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.57it/s]\n",
      "Epoch 4 train loss: 1.162736572265625\n",
      "test_batch (Avg. Loss 1.130, Accuracy 59.7): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 192.97it/s]\n",
      "--- EPOCH 6/35 ---\n",
      "train_batch (Avg. Loss 1.072, Accuracy 62.3): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.60it/s]\n",
      "Epoch 5 train loss: 1.07157470703125\n",
      "test_batch (Avg. Loss 1.081, Accuracy 61.4): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 192.03it/s]\n",
      "--- EPOCH 7/35 ---\n",
      "train_batch (Avg. Loss 0.996, Accuracy 64.9): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.58it/s]\n",
      "Epoch 6 train loss: 0.9962997436523438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_batch (Avg. Loss 1.044, Accuracy 62.6): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 195.32it/s]\n",
      "--- EPOCH 8/35 ---\n",
      "train_batch (Avg. Loss 0.928, Accuracy 67.6): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 52.39it/s]\n",
      "Epoch 7 train loss: 0.9282783203125\n",
      "test_batch (Avg. Loss 1.008, Accuracy 64.0): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 195.18it/s]\n",
      "--- EPOCH 9/35 ---\n",
      "train_batch (Avg. Loss 0.865, Accuracy 69.7): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 52.26it/s]\n",
      "Epoch 8 train loss: 0.864942626953125\n",
      "test_batch (Avg. Loss 0.984, Accuracy 65.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 194.91it/s]\n",
      "--- EPOCH 10/35 ---\n",
      "train_batch (Avg. Loss 0.803, Accuracy 72.1): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 52.13it/s]\n",
      "Epoch 9 train loss: 0.8028240356445312\n",
      "test_batch (Avg. Loss 0.967, Accuracy 66.1): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 196.51it/s]\n",
      "--- EPOCH 11/35 ---\n",
      "train_batch (Avg. Loss 0.743, Accuracy 74.2): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.98it/s]\n",
      "Epoch 10 train loss: 0.7432362670898438\n",
      "test_batch (Avg. Loss 0.942, Accuracy 67.1): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 182.45it/s]\n",
      "--- EPOCH 12/35 ---\n",
      "train_batch (Avg. Loss 0.685, Accuracy 76.3): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 52.24it/s]\n",
      "Epoch 11 train loss: 0.6853018188476563\n",
      "test_batch (Avg. Loss 0.936, Accuracy 68.0): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 194.54it/s]\n",
      "--- EPOCH 13/35 ---\n",
      "train_batch (Avg. Loss 0.627, Accuracy 78.4): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 52.09it/s]\n",
      "Epoch 12 train loss: 0.6265379638671875\n",
      "test_batch (Avg. Loss 0.953, Accuracy 68.0): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 197.82it/s]\n",
      "--- EPOCH 14/35 ---\n",
      "train_batch (Avg. Loss 0.570, Accuracy 80.5): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 52.41it/s]\n",
      "Epoch 13 train loss: 0.56982568359375\n",
      "test_batch (Avg. Loss 0.991, Accuracy 68.1): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 198.10it/s]\n",
      "--- EPOCH 15/35 ---\n",
      "train_batch (Avg. Loss 0.512, Accuracy 82.7): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.16it/s]\n",
      "Epoch 14 train loss: 0.5120477905273437\n",
      "test_batch (Avg. Loss 1.012, Accuracy 68.1): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 193.73it/s]\n",
      "--- EPOCH 16/35 ---\n",
      "train_batch (Avg. Loss 0.456, Accuracy 84.7): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.64it/s]\n",
      "Epoch 15 train loss: 0.4562604064941406\n",
      "test_batch (Avg. Loss 1.075, Accuracy 67.9): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 194.00it/s]\n",
      "--- EPOCH 17/35 ---\n",
      "train_batch (Avg. Loss 0.403, Accuracy 86.5): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.65it/s]\n",
      "Epoch 16 train loss: 0.4034022216796875\n",
      "test_batch (Avg. Loss 1.094, Accuracy 68.9): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 195.04it/s]\n",
      "*** Output file ./results\\test_run_L2_K64.json written\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "--- EPOCH 1/35 ---\n",
      "train_batch (Avg. Loss 2.017, Accuracy 27.6): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 63.39it/s]\n",
      "Epoch 0 train loss: 2.017162353515625\n",
      "test_batch (Avg. Loss 1.810, Accuracy 36.1): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 192.26it/s]\n",
      "--- EPOCH 2/35 ---\n",
      "train_batch (Avg. Loss 1.702, Accuracy 39.6): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 63.37it/s]\n",
      "Epoch 1 train loss: 1.701745849609375\n",
      "test_batch (Avg. Loss 1.587, Accuracy 43.5): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 192.66it/s]\n",
      "--- EPOCH 3/35 ---\n",
      "train_batch (Avg. Loss 1.539, Accuracy 45.0): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 63.22it/s]\n",
      "Epoch 2 train loss: 1.538695556640625\n",
      "test_batch (Avg. Loss 1.448, Accuracy 48.0): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 183.98it/s]\n",
      "--- EPOCH 4/35 ---\n",
      "train_batch (Avg. Loss 1.425, Accuracy 49.3): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 63.80it/s]\n",
      "Epoch 3 train loss: 1.424695556640625\n",
      "test_batch (Avg. Loss 1.362, Accuracy 51.7): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 201.70it/s]\n",
      "--- EPOCH 5/35 ---\n",
      "train_batch (Avg. Loss 1.343, Accuracy 52.2): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 66.35it/s]\n",
      "Epoch 4 train loss: 1.3432869873046875\n",
      "test_batch (Avg. Loss 1.298, Accuracy 54.1): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 206.34it/s]\n",
      "--- EPOCH 6/35 ---\n",
      "train_batch (Avg. Loss 1.275, Accuracy 54.7): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 66.45it/s]\n",
      "Epoch 5 train loss: 1.2751097412109376\n",
      "test_batch (Avg. Loss 1.256, Accuracy 55.7): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 194.72it/s]\n",
      "--- EPOCH 7/35 ---\n",
      "train_batch (Avg. Loss 1.215, Accuracy 56.8): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 65.49it/s]\n",
      "Epoch 6 train loss: 1.2153167724609375\n",
      "test_batch (Avg. Loss 1.216, Accuracy 57.0): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 201.70it/s]\n",
      "--- EPOCH 8/35 ---\n",
      "train_batch (Avg. Loss 1.157, Accuracy 59.0): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 63.74it/s]\n",
      "Epoch 7 train loss: 1.15677783203125\n",
      "test_batch (Avg. Loss 1.177, Accuracy 58.5): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 191.24it/s]\n",
      "--- EPOCH 9/35 ---\n",
      "train_batch (Avg. Loss 1.099, Accuracy 61.2): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 62.80it/s]\n",
      "Epoch 8 train loss: 1.0988001708984374\n",
      "test_batch (Avg. Loss 1.145, Accuracy 59.5): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 188.69it/s]\n",
      "--- EPOCH 10/35 ---\n",
      "train_batch (Avg. Loss 1.041, Accuracy 63.2): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 62.70it/s]\n",
      "Epoch 9 train loss: 1.040923095703125\n",
      "test_batch (Avg. Loss 1.120, Accuracy 60.6): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 189.20it/s]\n",
      "--- EPOCH 11/35 ---\n",
      "train_batch (Avg. Loss 0.984, Accuracy 65.2): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 62.90it/s]\n",
      "Epoch 10 train loss: 0.9844777221679688\n",
      "test_batch (Avg. Loss 1.095, Accuracy 61.7): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 191.33it/s]\n",
      "--- EPOCH 12/35 ---\n",
      "train_batch (Avg. Loss 0.929, Accuracy 67.2): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 62.67it/s]\n",
      "Epoch 11 train loss: 0.9285001220703125\n",
      "test_batch (Avg. Loss 1.073, Accuracy 62.7): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 192.39it/s]\n",
      "--- EPOCH 13/35 ---\n",
      "train_batch (Avg. Loss 0.872, Accuracy 69.5): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 62.82it/s]\n",
      "Epoch 12 train loss: 0.8722440185546875\n",
      "test_batch (Avg. Loss 1.074, Accuracy 62.8): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 191.02it/s]\n",
      "--- EPOCH 14/35 ---\n",
      "train_batch (Avg. Loss 0.816, Accuracy 71.5): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 63.05it/s]\n",
      "Epoch 13 train loss: 0.8157686767578125\n",
      "test_batch (Avg. Loss 1.085, Accuracy 63.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 199.38it/s]\n",
      "--- EPOCH 15/35 ---\n",
      "train_batch (Avg. Loss 0.758, Accuracy 73.7): 100%|████████████████████████████████| 1000/1000 [00:14<00:00, 66.90it/s]\n",
      "Epoch 14 train loss: 0.7581486206054687\n",
      "test_batch (Avg. Loss 1.085, Accuracy 63.6): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 186.49it/s]\n",
      "--- EPOCH 16/35 ---\n",
      "train_batch (Avg. Loss 0.701, Accuracy 75.6): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 62.63it/s]\n",
      "Epoch 15 train loss: 0.7007191162109375\n",
      "test_batch (Avg. Loss 1.073, Accuracy 64.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 188.65it/s]\n",
      "--- EPOCH 17/35 ---\n",
      "train_batch (Avg. Loss 0.645, Accuracy 77.7): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 63.26it/s]\n",
      "Epoch 16 train loss: 0.645174072265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_batch (Avg. Loss 1.086, Accuracy 64.9): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 192.88it/s]\n",
      "--- EPOCH 18/35 ---\n",
      "train_batch (Avg. Loss 0.592, Accuracy 79.6): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 63.02it/s]\n",
      "Epoch 17 train loss: 0.5920781860351563\n",
      "test_batch (Avg. Loss 1.121, Accuracy 64.9): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 187.88it/s]\n",
      "--- EPOCH 19/35 ---\n",
      "train_batch (Avg. Loss 0.538, Accuracy 81.8): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 62.95it/s]\n",
      "Epoch 18 train loss: 0.5379939575195313\n",
      "test_batch (Avg. Loss 1.165, Accuracy 65.0): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 201.36it/s]\n",
      "--- EPOCH 20/35 ---\n",
      "train_batch (Avg. Loss 0.484, Accuracy 83.7): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 66.63it/s]\n",
      "Epoch 19 train loss: 0.48441650390625\n",
      "test_batch (Avg. Loss 1.212, Accuracy 65.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 208.09it/s]\n",
      "--- EPOCH 21/35 ---\n",
      "train_batch (Avg. Loss 0.435, Accuracy 85.5): 100%|████████████████████████████████| 1000/1000 [00:15<00:00, 66.53it/s]\n",
      "Epoch 20 train loss: 0.4347128601074219\n",
      "test_batch (Avg. Loss 1.294, Accuracy 64.8): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 201.75it/s]\n",
      "*** Output file ./results\\test_run_L4_K32.json written\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "--- EPOCH 1/35 ---\n",
      "train_batch (Avg. Loss 1.935, Accuracy 30.3): 100%|████████████████████████████████| 1000/1000 [00:21<00:00, 45.98it/s]\n",
      "Epoch 0 train loss: 1.934946533203125\n",
      "test_batch (Avg. Loss 1.720, Accuracy 39.0): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 201.99it/s]\n",
      "--- EPOCH 2/35 ---\n",
      "train_batch (Avg. Loss 1.618, Accuracy 42.3): 100%|████████████████████████████████| 1000/1000 [00:21<00:00, 46.28it/s]\n",
      "Epoch 1 train loss: 1.6176895751953124\n",
      "test_batch (Avg. Loss 1.512, Accuracy 46.4): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 189.29it/s]\n",
      "--- EPOCH 3/35 ---\n",
      "train_batch (Avg. Loss 1.462, Accuracy 47.8): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 45.40it/s]\n",
      "Epoch 2 train loss: 1.4624508056640626\n",
      "test_batch (Avg. Loss 1.369, Accuracy 51.4): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 204.27it/s]\n",
      "--- EPOCH 4/35 ---\n",
      "train_batch (Avg. Loss 1.346, Accuracy 52.1): 100%|████████████████████████████████| 1000/1000 [00:21<00:00, 47.15it/s]\n",
      "Epoch 3 train loss: 1.3456346435546875\n",
      "test_batch (Avg. Loss 1.276, Accuracy 54.3): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 205.57it/s]\n",
      "--- EPOCH 5/35 ---\n",
      "train_batch (Avg. Loss 1.249, Accuracy 55.5): 100%|████████████████████████████████| 1000/1000 [00:21<00:00, 46.85it/s]\n",
      "Epoch 4 train loss: 1.2492060546875\n",
      "test_batch (Avg. Loss 1.206, Accuracy 57.2): 100%|██████████████████████████████████| 834/834 [00:03<00:00, 210.35it/s]\n",
      "--- EPOCH 6/35 ---\n",
      "train_batch (Avg. Loss 1.161, Accuracy 58.8): 100%|████████████████████████████████| 1000/1000 [00:21<00:00, 45.98it/s]\n",
      "Epoch 5 train loss: 1.161050048828125\n",
      "test_batch (Avg. Loss 1.142, Accuracy 59.5): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 188.05it/s]\n",
      "--- EPOCH 7/35 ---\n",
      "train_batch (Avg. Loss 1.081, Accuracy 61.7): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.41it/s]\n",
      "Epoch 6 train loss: 1.0805389404296875\n",
      "test_batch (Avg. Loss 1.101, Accuracy 60.8): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 188.82it/s]\n",
      "--- EPOCH 8/35 ---\n",
      "train_batch (Avg. Loss 1.010, Accuracy 64.4): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.30it/s]\n",
      "Epoch 7 train loss: 1.009979736328125\n",
      "test_batch (Avg. Loss 1.060, Accuracy 62.4): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 186.66it/s]\n",
      "--- EPOCH 9/35 ---\n",
      "train_batch (Avg. Loss 0.944, Accuracy 67.0): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.31it/s]\n",
      "Epoch 8 train loss: 0.944125244140625\n",
      "test_batch (Avg. Loss 1.028, Accuracy 63.7): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 190.41it/s]\n",
      "--- EPOCH 10/35 ---\n",
      "train_batch (Avg. Loss 0.881, Accuracy 69.4): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.25it/s]\n",
      "Epoch 9 train loss: 0.8805110473632812\n",
      "test_batch (Avg. Loss 0.993, Accuracy 65.0): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 188.52it/s]\n",
      "--- EPOCH 11/35 ---\n",
      "train_batch (Avg. Loss 0.815, Accuracy 71.7): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.31it/s]\n",
      "Epoch 10 train loss: 0.814776123046875\n",
      "test_batch (Avg. Loss 0.970, Accuracy 66.4): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 188.77it/s]\n",
      "--- EPOCH 12/35 ---\n",
      "train_batch (Avg. Loss 0.751, Accuracy 74.0): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.45it/s]\n",
      "Epoch 11 train loss: 0.7513678588867188\n",
      "test_batch (Avg. Loss 0.964, Accuracy 66.6): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 187.71it/s]\n",
      "--- EPOCH 13/35 ---\n",
      "train_batch (Avg. Loss 0.688, Accuracy 76.2): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.44it/s]\n",
      "Epoch 12 train loss: 0.6881962280273437\n",
      "test_batch (Avg. Loss 0.962, Accuracy 67.0): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 190.02it/s]\n",
      "--- EPOCH 14/35 ---\n",
      "train_batch (Avg. Loss 0.625, Accuracy 78.6): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.42it/s]\n",
      "Epoch 13 train loss: 0.6254833374023437\n",
      "test_batch (Avg. Loss 0.975, Accuracy 67.2): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 184.80it/s]\n",
      "--- EPOCH 15/35 ---\n",
      "train_batch (Avg. Loss 0.562, Accuracy 81.0): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.33it/s]\n",
      "Epoch 14 train loss: 0.56234912109375\n",
      "test_batch (Avg. Loss 1.007, Accuracy 67.6): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 186.24it/s]\n",
      "--- EPOCH 16/35 ---\n",
      "train_batch (Avg. Loss 0.499, Accuracy 83.3): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.36it/s]\n",
      "Epoch 15 train loss: 0.4989422302246094\n",
      "test_batch (Avg. Loss 1.062, Accuracy 67.3): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 187.21it/s]\n",
      "--- EPOCH 17/35 ---\n",
      "train_batch (Avg. Loss 0.438, Accuracy 85.6): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.27it/s]\n",
      "Epoch 16 train loss: 0.4380950012207031\n",
      "test_batch (Avg. Loss 1.163, Accuracy 66.8): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 186.16it/s]\n",
      "--- EPOCH 18/35 ---\n",
      "train_batch (Avg. Loss 0.384, Accuracy 87.5): 100%|████████████████████████████████| 1000/1000 [00:22<00:00, 44.30it/s]\n",
      "Epoch 17 train loss: 0.3839923095703125\n",
      "test_batch (Avg. Loss 1.315, Accuracy 65.3): 100%|██████████████████████████████████| 834/834 [00:04<00:00, 185.62it/s]\n",
      "*** Output file ./results\\test_run_L4_K64.json written\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "--- EPOCH 1/35 ---\n",
      "train_batch (Avg. Loss 2.156, Accuracy 21.9): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.49it/s]\n",
      "Epoch 0 train loss: 2.156242919921875\n",
      "test_batch (Avg. Loss 1.954, Accuracy 29.8): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 154.38it/s]\n",
      "--- EPOCH 2/35 ---\n",
      "train_batch (Avg. Loss 1.827, Accuracy 34.7): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.83it/s]\n",
      "Epoch 1 train loss: 1.8271246337890625\n",
      "test_batch (Avg. Loss 1.724, Accuracy 38.9): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 150.32it/s]\n",
      "--- EPOCH 3/35 ---\n",
      "train_batch (Avg. Loss 1.626, Accuracy 41.4): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.82it/s]\n",
      "Epoch 2 train loss: 1.6263997802734376\n",
      "test_batch (Avg. Loss 1.530, Accuracy 45.1): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 152.94it/s]\n",
      "--- EPOCH 4/35 ---\n",
      "train_batch (Avg. Loss 1.501, Accuracy 46.2): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.57it/s]\n",
      "Epoch 3 train loss: 1.5008038330078124\n",
      "test_batch (Avg. Loss 1.445, Accuracy 48.2): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 153.42it/s]\n",
      "--- EPOCH 5/35 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch (Avg. Loss 1.420, Accuracy 49.3): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.11it/s]\n",
      "Epoch 4 train loss: 1.4203165283203125\n",
      "test_batch (Avg. Loss 1.369, Accuracy 51.0): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 155.13it/s]\n",
      "--- EPOCH 6/35 ---\n",
      "train_batch (Avg. Loss 1.346, Accuracy 52.0): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.86it/s]\n",
      "Epoch 5 train loss: 1.3462724609375\n",
      "test_batch (Avg. Loss 1.294, Accuracy 54.1): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 155.27it/s]\n",
      "--- EPOCH 7/35 ---\n",
      "train_batch (Avg. Loss 1.274, Accuracy 54.7): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.10it/s]\n",
      "Epoch 6 train loss: 1.273728759765625\n",
      "test_batch (Avg. Loss 1.231, Accuracy 56.3): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 154.27it/s]\n",
      "--- EPOCH 8/35 ---\n",
      "train_batch (Avg. Loss 1.202, Accuracy 57.2): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.50it/s]\n",
      "Epoch 7 train loss: 1.2016849365234374\n",
      "test_batch (Avg. Loss 1.174, Accuracy 58.6): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 154.21it/s]\n",
      "--- EPOCH 9/35 ---\n",
      "train_batch (Avg. Loss 1.130, Accuracy 59.9): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.10it/s]\n",
      "Epoch 8 train loss: 1.1298062744140625\n",
      "test_batch (Avg. Loss 1.122, Accuracy 60.5): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 153.64it/s]\n",
      "--- EPOCH 10/35 ---\n",
      "train_batch (Avg. Loss 1.058, Accuracy 62.6): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.75it/s]\n",
      "Epoch 9 train loss: 1.0584249267578125\n",
      "test_batch (Avg. Loss 1.074, Accuracy 62.4): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 155.94it/s]\n",
      "--- EPOCH 11/35 ---\n",
      "train_batch (Avg. Loss 0.987, Accuracy 65.3): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.94it/s]\n",
      "Epoch 10 train loss: 0.9865426025390625\n",
      "test_batch (Avg. Loss 1.033, Accuracy 64.0): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 152.10it/s]\n",
      "--- EPOCH 12/35 ---\n",
      "train_batch (Avg. Loss 0.920, Accuracy 67.9): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.94it/s]\n",
      "Epoch 11 train loss: 0.9196456909179688\n",
      "test_batch (Avg. Loss 1.007, Accuracy 65.0): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 156.61it/s]\n",
      "--- EPOCH 13/35 ---\n",
      "train_batch (Avg. Loss 0.860, Accuracy 70.0): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.04it/s]\n",
      "Epoch 12 train loss: 0.8603877563476563\n",
      "test_batch (Avg. Loss 0.995, Accuracy 65.5): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 154.30it/s]\n",
      "--- EPOCH 14/35 ---\n",
      "train_batch (Avg. Loss 0.807, Accuracy 71.8): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.00it/s]\n",
      "Epoch 13 train loss: 0.80670166015625\n",
      "test_batch (Avg. Loss 0.984, Accuracy 66.4): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 155.04it/s]\n",
      "--- EPOCH 15/35 ---\n",
      "train_batch (Avg. Loss 0.758, Accuracy 73.6): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.75it/s]\n",
      "Epoch 14 train loss: 0.7577033081054687\n",
      "test_batch (Avg. Loss 0.961, Accuracy 67.2): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 157.06it/s]\n",
      "--- EPOCH 16/35 ---\n",
      "train_batch (Avg. Loss 0.712, Accuracy 75.3): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.84it/s]\n",
      "Epoch 15 train loss: 0.71153955078125\n",
      "test_batch (Avg. Loss 0.947, Accuracy 67.8): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 153.58it/s]\n",
      "--- EPOCH 17/35 ---\n",
      "train_batch (Avg. Loss 0.665, Accuracy 77.0): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 51.16it/s]\n",
      "Epoch 16 train loss: 0.6652640991210937\n",
      "test_batch (Avg. Loss 0.929, Accuracy 68.4): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 156.32it/s]\n",
      "--- EPOCH 18/35 ---\n",
      "train_batch (Avg. Loss 0.622, Accuracy 78.5): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.86it/s]\n",
      "Epoch 17 train loss: 0.6216832275390625\n",
      "test_batch (Avg. Loss 0.926, Accuracy 68.7): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 153.11it/s]\n",
      "--- EPOCH 19/35 ---\n",
      "train_batch (Avg. Loss 0.579, Accuracy 80.2): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.93it/s]\n",
      "Epoch 18 train loss: 0.5794161376953125\n",
      "test_batch (Avg. Loss 0.939, Accuracy 69.1): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 155.24it/s]\n",
      "--- EPOCH 20/35 ---\n",
      "train_batch (Avg. Loss 0.541, Accuracy 81.3): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.81it/s]\n",
      "Epoch 19 train loss: 0.5408609619140625\n",
      "test_batch (Avg. Loss 0.973, Accuracy 68.8): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 153.02it/s]\n",
      "--- EPOCH 21/35 ---\n",
      "train_batch (Avg. Loss 0.499, Accuracy 83.0): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.71it/s]\n",
      "Epoch 20 train loss: 0.49898184204101564\n",
      "test_batch (Avg. Loss 1.013, Accuracy 68.8): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 153.02it/s]\n",
      "--- EPOCH 22/35 ---\n",
      "train_batch (Avg. Loss 0.461, Accuracy 84.2): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.72it/s]\n",
      "Epoch 21 train loss: 0.46149298095703123\n",
      "test_batch (Avg. Loss 1.055, Accuracy 68.8): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 156.09it/s]\n",
      "--- EPOCH 23/35 ---\n",
      "train_batch (Avg. Loss 0.427, Accuracy 85.4): 100%|████████████████████████████████| 1000/1000 [00:19<00:00, 50.94it/s]\n",
      "Epoch 22 train loss: 0.427265625\n",
      "test_batch (Avg. Loss 1.154, Accuracy 67.9): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 152.96it/s]\n",
      "*** Output file ./results\\test_run_L8_K32.json written\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "--- EPOCH 1/35 ---\n",
      "train_batch (Avg. Loss 2.077, Accuracy 24.9): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 38.07it/s]\n",
      "Epoch 0 train loss: 2.077022705078125\n",
      "test_batch (Avg. Loss 1.887, Accuracy 32.8): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 154.55it/s]\n",
      "--- EPOCH 2/35 ---\n",
      "train_batch (Avg. Loss 1.743, Accuracy 37.7): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 37.86it/s]\n",
      "Epoch 1 train loss: 1.7426343994140625\n",
      "test_batch (Avg. Loss 1.632, Accuracy 42.1): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 152.52it/s]\n",
      "--- EPOCH 3/35 ---\n",
      "train_batch (Avg. Loss 1.537, Accuracy 45.0): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 37.92it/s]\n",
      "Epoch 2 train loss: 1.537441162109375\n",
      "test_batch (Avg. Loss 1.449, Accuracy 47.6): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 153.02it/s]\n",
      "--- EPOCH 4/35 ---\n",
      "train_batch (Avg. Loss 1.403, Accuracy 49.8): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 37.91it/s]\n",
      "Epoch 3 train loss: 1.4025595703125\n",
      "test_batch (Avg. Loss 1.332, Accuracy 51.8): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 154.44it/s]\n",
      "--- EPOCH 5/35 ---\n",
      "train_batch (Avg. Loss 1.293, Accuracy 54.0): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 37.91it/s]\n",
      "Epoch 4 train loss: 1.293021728515625\n",
      "test_batch (Avg. Loss 1.231, Accuracy 56.1): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 151.03it/s]\n",
      "--- EPOCH 6/35 ---\n",
      "train_batch (Avg. Loss 1.187, Accuracy 57.9): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 37.42it/s]\n",
      "Epoch 5 train loss: 1.1869334716796875\n",
      "test_batch (Avg. Loss 1.126, Accuracy 60.1): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 142.56it/s]\n",
      "--- EPOCH 7/35 ---\n",
      "train_batch (Avg. Loss 1.084, Accuracy 61.7): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 37.55it/s]\n",
      "Epoch 6 train loss: 1.0835858154296876\n",
      "test_batch (Avg. Loss 1.053, Accuracy 62.7): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 152.41it/s]\n",
      "--- EPOCH 8/35 ---\n",
      "train_batch (Avg. Loss 0.992, Accuracy 65.1): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 37.79it/s]\n",
      "Epoch 7 train loss: 0.9923370971679687\n",
      "test_batch (Avg. Loss 1.005, Accuracy 64.8): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 150.56it/s]\n",
      "--- EPOCH 9/35 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch (Avg. Loss 0.913, Accuracy 68.0): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 38.32it/s]\n",
      "Epoch 8 train loss: 0.9129404296875\n",
      "test_batch (Avg. Loss 0.953, Accuracy 67.0): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 153.22it/s]\n",
      "--- EPOCH 10/35 ---\n",
      "train_batch (Avg. Loss 0.845, Accuracy 70.5): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 38.21it/s]\n",
      "Epoch 9 train loss: 0.8453770141601562\n",
      "test_batch (Avg. Loss 0.915, Accuracy 68.5): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 149.83it/s]\n",
      "--- EPOCH 11/35 ---\n",
      "train_batch (Avg. Loss 0.785, Accuracy 72.7): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 38.26it/s]\n",
      "Epoch 10 train loss: 0.78482373046875\n",
      "test_batch (Avg. Loss 0.883, Accuracy 69.8): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 153.30it/s]\n",
      "--- EPOCH 12/35 ---\n",
      "train_batch (Avg. Loss 0.729, Accuracy 74.6): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 38.19it/s]\n",
      "Epoch 11 train loss: 0.7289205932617188\n",
      "test_batch (Avg. Loss 0.860, Accuracy 70.6): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 154.35it/s]\n",
      "--- EPOCH 13/35 ---\n",
      "train_batch (Avg. Loss 0.674, Accuracy 76.6): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 38.01it/s]\n",
      "Epoch 12 train loss: 0.6743223266601562\n",
      "test_batch (Avg. Loss 0.841, Accuracy 71.2): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 154.55it/s]\n",
      "--- EPOCH 14/35 ---\n",
      "train_batch (Avg. Loss 0.620, Accuracy 78.5): 100%|████████████████████████████████| 1000/1000 [00:25<00:00, 38.77it/s]\n",
      "Epoch 13 train loss: 0.6197437744140625\n",
      "test_batch (Avg. Loss 0.826, Accuracy 72.1): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 160.29it/s]\n",
      "--- EPOCH 15/35 ---\n",
      "train_batch (Avg. Loss 0.567, Accuracy 80.2): 100%|████████████████████████████████| 1000/1000 [00:25<00:00, 39.24it/s]\n",
      "Epoch 14 train loss: 0.5673726806640625\n",
      "test_batch (Avg. Loss 0.827, Accuracy 72.6): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 161.00it/s]\n",
      "--- EPOCH 16/35 ---\n",
      "train_batch (Avg. Loss 0.513, Accuracy 82.3): 100%|████████████████████████████████| 1000/1000 [00:25<00:00, 38.87it/s]\n",
      "Epoch 15 train loss: 0.513457275390625\n",
      "test_batch (Avg. Loss 0.842, Accuracy 73.0): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 149.78it/s]\n",
      "--- EPOCH 17/35 ---\n",
      "train_batch (Avg. Loss 0.462, Accuracy 84.3): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 37.91it/s]\n",
      "Epoch 16 train loss: 0.46167416381835935\n",
      "test_batch (Avg. Loss 0.864, Accuracy 73.0): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 148.76it/s]\n",
      "--- EPOCH 18/35 ---\n",
      "train_batch (Avg. Loss 0.411, Accuracy 86.3): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 38.17it/s]\n",
      "Epoch 17 train loss: 0.41068658447265627\n",
      "test_batch (Avg. Loss 0.889, Accuracy 73.2): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 152.52it/s]\n",
      "--- EPOCH 19/35 ---\n",
      "train_batch (Avg. Loss 0.363, Accuracy 88.1): 100%|████████████████████████████████| 1000/1000 [00:26<00:00, 38.26it/s]\n",
      "Epoch 18 train loss: 0.3632273864746094\n",
      "test_batch (Avg. Loss 0.938, Accuracy 72.5): 100%|██████████████████████████████████| 834/834 [00:05<00:00, 152.80it/s]\n",
      "*** Output file ./results\\test_run_L8_K64.json written\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "--- EPOCH 1/35 ---\n",
      "train_batch (Avg. Loss 2.301, Accuracy 9.6): 100%|█████████████████████████████████| 1000/1000 [00:28<00:00, 35.41it/s]\n",
      "Epoch 0 train loss: 2.301356689453125\n",
      "test_batch (Avg. Loss 2.296, Accuracy 10.0): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 107.63it/s]\n",
      "--- EPOCH 2/35 ---\n",
      "train_batch (Avg. Loss 2.284, Accuracy 13.7): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.42it/s]\n",
      "Epoch 1 train loss: 2.284232177734375\n",
      "test_batch (Avg. Loss 2.260, Accuracy 16.5): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.63it/s]\n",
      "--- EPOCH 3/35 ---\n",
      "train_batch (Avg. Loss 2.156, Accuracy 19.6): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.58it/s]\n",
      "Epoch 2 train loss: 2.156344970703125\n",
      "test_batch (Avg. Loss 2.043, Accuracy 22.9): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.23it/s]\n",
      "--- EPOCH 4/35 ---\n",
      "train_batch (Avg. Loss 1.994, Accuracy 25.5): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.26it/s]\n",
      "Epoch 3 train loss: 1.9935145263671874\n",
      "test_batch (Avg. Loss 1.906, Accuracy 29.8): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 109.29it/s]\n",
      "--- EPOCH 5/35 ---\n",
      "train_batch (Avg. Loss 1.803, Accuracy 33.2): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 34.97it/s]\n",
      "Epoch 4 train loss: 1.8027633056640624\n",
      "test_batch (Avg. Loss 1.748, Accuracy 34.9): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 111.23it/s]\n",
      "--- EPOCH 6/35 ---\n",
      "train_batch (Avg. Loss 1.638, Accuracy 39.5): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.36it/s]\n",
      "Epoch 5 train loss: 1.6382579345703125\n",
      "test_batch (Avg. Loss 1.602, Accuracy 40.7): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 109.22it/s]\n",
      "--- EPOCH 7/35 ---\n",
      "train_batch (Avg. Loss 1.522, Accuracy 44.0): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 34.99it/s]\n",
      "Epoch 6 train loss: 1.5223885498046874\n",
      "test_batch (Avg. Loss 1.501, Accuracy 44.7): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 109.37it/s]\n",
      "--- EPOCH 8/35 ---\n",
      "train_batch (Avg. Loss 1.439, Accuracy 47.5): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 34.89it/s]\n",
      "Epoch 7 train loss: 1.4385777587890625\n",
      "test_batch (Avg. Loss 1.436, Accuracy 47.1): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 112.09it/s]\n",
      "--- EPOCH 9/35 ---\n",
      "train_batch (Avg. Loss 1.367, Accuracy 50.3): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.43it/s]\n",
      "Epoch 8 train loss: 1.367194580078125\n",
      "test_batch (Avg. Loss 1.371, Accuracy 49.9): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 113.86it/s]\n",
      "--- EPOCH 10/35 ---\n",
      "train_batch (Avg. Loss 1.300, Accuracy 52.8): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.41it/s]\n",
      "Epoch 9 train loss: 1.29989453125\n",
      "test_batch (Avg. Loss 1.315, Accuracy 52.2): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 114.36it/s]\n",
      "--- EPOCH 11/35 ---\n",
      "train_batch (Avg. Loss 1.238, Accuracy 55.2): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.40it/s]\n",
      "Epoch 10 train loss: 1.2375838623046875\n",
      "test_batch (Avg. Loss 1.254, Accuracy 54.7): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 112.51it/s]\n",
      "--- EPOCH 12/35 ---\n",
      "train_batch (Avg. Loss 1.185, Accuracy 57.2): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.47it/s]\n",
      "Epoch 11 train loss: 1.18479150390625\n",
      "test_batch (Avg. Loss 1.203, Accuracy 56.7): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 110.63it/s]\n",
      "--- EPOCH 13/35 ---\n",
      "train_batch (Avg. Loss 1.135, Accuracy 59.0): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.26it/s]\n",
      "Epoch 12 train loss: 1.13544189453125\n",
      "test_batch (Avg. Loss 1.165, Accuracy 58.5): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 113.66it/s]\n",
      "--- EPOCH 14/35 ---\n",
      "train_batch (Avg. Loss 1.090, Accuracy 60.9): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.36it/s]\n",
      "Epoch 13 train loss: 1.0896611328125\n",
      "test_batch (Avg. Loss 1.115, Accuracy 60.2): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 112.72it/s]\n",
      "--- EPOCH 15/35 ---\n",
      "train_batch (Avg. Loss 1.046, Accuracy 62.5): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.40it/s]\n",
      "Epoch 14 train loss: 1.0458104248046876\n",
      "test_batch (Avg. Loss 1.079, Accuracy 61.5): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 114.17it/s]\n",
      "--- EPOCH 16/35 ---\n",
      "train_batch (Avg. Loss 1.007, Accuracy 64.0): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.57it/s]\n",
      "Epoch 15 train loss: 1.0066953125\n",
      "test_batch (Avg. Loss 1.052, Accuracy 62.6): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 112.89it/s]\n",
      "--- EPOCH 17/35 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch (Avg. Loss 0.969, Accuracy 65.4): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.18it/s]\n",
      "Epoch 16 train loss: 0.9690750122070313\n",
      "test_batch (Avg. Loss 1.040, Accuracy 63.3): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 104.32it/s]\n",
      "--- EPOCH 18/35 ---\n",
      "train_batch (Avg. Loss 0.936, Accuracy 66.7): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.42it/s]\n",
      "Epoch 17 train loss: 0.9364315185546875\n",
      "test_batch (Avg. Loss 1.037, Accuracy 63.5): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.77it/s]\n",
      "--- EPOCH 19/35 ---\n",
      "train_batch (Avg. Loss 0.903, Accuracy 67.9): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.36it/s]\n",
      "Epoch 18 train loss: 0.9033301391601563\n",
      "test_batch (Avg. Loss 1.030, Accuracy 63.9): 100%|██████████████████████████████████| 834/834 [00:08<00:00, 103.72it/s]\n",
      "--- EPOCH 20/35 ---\n",
      "train_batch (Avg. Loss 0.872, Accuracy 69.0): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.43it/s]\n",
      "Epoch 19 train loss: 0.8724494018554687\n",
      "test_batch (Avg. Loss 1.019, Accuracy 64.8): 100%|██████████████████████████████████| 834/834 [00:08<00:00, 103.62it/s]\n",
      "--- EPOCH 21/35 ---\n",
      "train_batch (Avg. Loss 0.842, Accuracy 70.2): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.34it/s]\n",
      "Epoch 20 train loss: 0.8422498168945313\n",
      "test_batch (Avg. Loss 1.012, Accuracy 64.7): 100%|██████████████████████████████████| 834/834 [00:08<00:00, 103.88it/s]\n",
      "--- EPOCH 22/35 ---\n",
      "train_batch (Avg. Loss 0.813, Accuracy 71.4): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.41it/s]\n",
      "Epoch 21 train loss: 0.8125968627929687\n",
      "test_batch (Avg. Loss 1.011, Accuracy 65.0): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.23it/s]\n",
      "--- EPOCH 23/35 ---\n",
      "train_batch (Avg. Loss 0.784, Accuracy 72.5): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.53it/s]\n",
      "Epoch 22 train loss: 0.784254638671875\n",
      "test_batch (Avg. Loss 0.984, Accuracy 65.9): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 104.38it/s]\n",
      "--- EPOCH 24/35 ---\n",
      "train_batch (Avg. Loss 0.759, Accuracy 73.5): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.36it/s]\n",
      "Epoch 23 train loss: 0.7588081665039063\n",
      "test_batch (Avg. Loss 0.979, Accuracy 66.3): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.25it/s]\n",
      "--- EPOCH 25/35 ---\n",
      "train_batch (Avg. Loss 0.733, Accuracy 74.4): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.20it/s]\n",
      "Epoch 24 train loss: 0.7334435424804687\n",
      "test_batch (Avg. Loss 0.990, Accuracy 66.3): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.55it/s]\n",
      "--- EPOCH 26/35 ---\n",
      "train_batch (Avg. Loss 0.710, Accuracy 75.2): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.56it/s]\n",
      "Epoch 25 train loss: 0.70981103515625\n",
      "test_batch (Avg. Loss 0.968, Accuracy 67.0): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.45it/s]\n",
      "--- EPOCH 27/35 ---\n",
      "train_batch (Avg. Loss 0.685, Accuracy 76.1): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.29it/s]\n",
      "Epoch 26 train loss: 0.6850479125976563\n",
      "test_batch (Avg. Loss 0.963, Accuracy 67.5): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.05it/s]\n",
      "--- EPOCH 28/35 ---\n",
      "train_batch (Avg. Loss 0.663, Accuracy 76.8): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.55it/s]\n",
      "Epoch 27 train loss: 0.6627523803710937\n",
      "test_batch (Avg. Loss 0.960, Accuracy 67.8): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.45it/s]\n",
      "--- EPOCH 29/35 ---\n",
      "train_batch (Avg. Loss 0.641, Accuracy 77.6): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.00it/s]\n",
      "Epoch 28 train loss: 0.6411394653320313\n",
      "test_batch (Avg. Loss 0.947, Accuracy 68.6): 100%|██████████████████████████████████| 834/834 [00:08<00:00, 102.67it/s]\n",
      "--- EPOCH 30/35 ---\n",
      "train_batch (Avg. Loss 0.622, Accuracy 78.4): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 34.81it/s]\n",
      "Epoch 29 train loss: 0.622002197265625\n",
      "test_batch (Avg. Loss 0.962, Accuracy 68.4): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 111.89it/s]\n",
      "--- EPOCH 31/35 ---\n",
      "train_batch (Avg. Loss 0.604, Accuracy 79.1): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.21it/s]\n",
      "Epoch 30 train loss: 0.603625244140625\n",
      "test_batch (Avg. Loss 0.967, Accuracy 68.7): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 109.51it/s]\n",
      "--- EPOCH 32/35 ---\n",
      "train_batch (Avg. Loss 0.583, Accuracy 79.7): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.21it/s]\n",
      "Epoch 31 train loss: 0.5834775390625\n",
      "test_batch (Avg. Loss 0.974, Accuracy 68.7): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 112.25it/s]\n",
      "--- EPOCH 33/35 ---\n",
      "train_batch (Avg. Loss 0.562, Accuracy 80.4): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.24it/s]\n",
      "Epoch 32 train loss: 0.56172509765625\n",
      "test_batch (Avg. Loss 1.002, Accuracy 68.7): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 113.72it/s]\n",
      "--- EPOCH 34/35 ---\n",
      "train_batch (Avg. Loss 0.544, Accuracy 81.1): 100%|████████████████████████████████| 1000/1000 [00:28<00:00, 35.11it/s]\n",
      "Epoch 33 train loss: 0.5442102661132813\n",
      "test_batch (Avg. Loss 1.033, Accuracy 68.1): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 107.83it/s]\n",
      "*** Output file ./results\\test_run_L16_K32.json written\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "--- EPOCH 1/35 ---\n",
      "train_batch (Avg. Loss 2.295, Accuracy 12.2): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.60it/s]\n",
      "Epoch 0 train loss: 2.294579833984375\n",
      "test_batch (Avg. Loss 2.283, Accuracy 15.4): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.48it/s]\n",
      "--- EPOCH 2/35 ---\n",
      "train_batch (Avg. Loss 2.218, Accuracy 18.3): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.69it/s]\n",
      "Epoch 1 train loss: 2.218115478515625\n",
      "test_batch (Avg. Loss 2.051, Accuracy 24.0): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.14it/s]\n",
      "--- EPOCH 3/35 ---\n",
      "train_batch (Avg. Loss 1.953, Accuracy 28.8): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.67it/s]\n",
      "Epoch 2 train loss: 1.953003173828125\n",
      "test_batch (Avg. Loss 1.850, Accuracy 34.1): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.04it/s]\n",
      "--- EPOCH 4/35 ---\n",
      "train_batch (Avg. Loss 1.758, Accuracy 36.8): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.70it/s]\n",
      "Epoch 3 train loss: 1.757793701171875\n",
      "test_batch (Avg. Loss 1.654, Accuracy 40.8): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 104.25it/s]\n",
      "--- EPOCH 5/35 ---\n",
      "train_batch (Avg. Loss 1.603, Accuracy 42.4): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.66it/s]\n",
      "Epoch 4 train loss: 1.602589599609375\n",
      "test_batch (Avg. Loss 1.528, Accuracy 45.0): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 104.82it/s]\n",
      "--- EPOCH 6/35 ---\n",
      "train_batch (Avg. Loss 1.492, Accuracy 46.2): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.61it/s]\n",
      "Epoch 5 train loss: 1.492216552734375\n",
      "test_batch (Avg. Loss 1.437, Accuracy 48.5): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.53it/s]\n",
      "--- EPOCH 7/35 ---\n",
      "train_batch (Avg. Loss 1.414, Accuracy 48.8): 100%|████████████████████████████████| 1000/1000 [00:35<00:00, 28.48it/s]\n",
      "Epoch 6 train loss: 1.4137911376953125\n",
      "test_batch (Avg. Loss 1.365, Accuracy 51.1): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.71it/s]\n",
      "--- EPOCH 8/35 ---\n",
      "train_batch (Avg. Loss 1.349, Accuracy 51.3): 100%|████████████████████████████████| 1000/1000 [00:35<00:00, 28.55it/s]\n",
      "Epoch 7 train loss: 1.3491090087890625\n",
      "test_batch (Avg. Loss 1.310, Accuracy 53.1): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.91it/s]\n",
      "--- EPOCH 9/35 ---\n",
      "train_batch (Avg. Loss 1.288, Accuracy 53.5): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.63it/s]\n",
      "Epoch 8 train loss: 1.2878338623046874\n",
      "test_batch (Avg. Loss 1.262, Accuracy 55.4): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 10/35 ---\n",
      "train_batch (Avg. Loss 1.222, Accuracy 56.2): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.81it/s]\n",
      "Epoch 9 train loss: 1.2220479736328125\n",
      "test_batch (Avg. Loss 1.230, Accuracy 56.9): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.54it/s]\n",
      "--- EPOCH 11/35 ---\n",
      "train_batch (Avg. Loss 1.155, Accuracy 58.7): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.64it/s]\n",
      "Epoch 10 train loss: 1.15477978515625\n",
      "test_batch (Avg. Loss 1.175, Accuracy 58.4): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 107.56it/s]\n",
      "--- EPOCH 12/35 ---\n",
      "train_batch (Avg. Loss 1.091, Accuracy 61.2): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.78it/s]\n",
      "Epoch 11 train loss: 1.0911729736328124\n",
      "test_batch (Avg. Loss 1.112, Accuracy 60.6): 100%|██████████████████████████████████| 834/834 [00:08<00:00, 104.14it/s]\n",
      "--- EPOCH 13/35 ---\n",
      "train_batch (Avg. Loss 1.028, Accuracy 63.5): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.83it/s]\n",
      "Epoch 12 train loss: 1.027612060546875\n",
      "test_batch (Avg. Loss 1.040, Accuracy 63.2): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.18it/s]\n",
      "--- EPOCH 14/35 ---\n",
      "train_batch (Avg. Loss 0.967, Accuracy 65.9): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.69it/s]\n",
      "Epoch 13 train loss: 0.9665224609375\n",
      "test_batch (Avg. Loss 0.997, Accuracy 65.1): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 104.47it/s]\n",
      "--- EPOCH 15/35 ---\n",
      "train_batch (Avg. Loss 0.910, Accuracy 67.9): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.69it/s]\n",
      "Epoch 14 train loss: 0.9100479736328125\n",
      "test_batch (Avg. Loss 0.963, Accuracy 66.2): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 104.64it/s]\n",
      "--- EPOCH 16/35 ---\n",
      "train_batch (Avg. Loss 0.858, Accuracy 70.0): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.71it/s]\n",
      "Epoch 15 train loss: 0.8580896606445313\n",
      "test_batch (Avg. Loss 0.937, Accuracy 67.4): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.18it/s]\n",
      "--- EPOCH 17/35 ---\n",
      "train_batch (Avg. Loss 0.809, Accuracy 71.6): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.75it/s]\n",
      "Epoch 16 train loss: 0.8092028198242187\n",
      "test_batch (Avg. Loss 0.920, Accuracy 67.8): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 104.87it/s]\n",
      "--- EPOCH 18/35 ---\n",
      "train_batch (Avg. Loss 0.765, Accuracy 73.2): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.73it/s]\n",
      "Epoch 17 train loss: 0.7648656616210937\n",
      "test_batch (Avg. Loss 0.912, Accuracy 68.6): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.05it/s]\n",
      "--- EPOCH 19/35 ---\n",
      "train_batch (Avg. Loss 0.722, Accuracy 74.7): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.78it/s]\n",
      "Epoch 18 train loss: 0.72150341796875\n",
      "test_batch (Avg. Loss 0.917, Accuracy 68.8): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.36it/s]\n",
      "--- EPOCH 20/35 ---\n",
      "train_batch (Avg. Loss 0.679, Accuracy 76.4): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.71it/s]\n",
      "Epoch 19 train loss: 0.6791925659179687\n",
      "test_batch (Avg. Loss 0.907, Accuracy 69.6): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 104.58it/s]\n",
      "--- EPOCH 21/35 ---\n",
      "train_batch (Avg. Loss 0.638, Accuracy 77.8): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.73it/s]\n",
      "Epoch 20 train loss: 0.6384970703125\n",
      "test_batch (Avg. Loss 0.907, Accuracy 69.6): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 104.83it/s]\n",
      "--- EPOCH 22/35 ---\n",
      "train_batch (Avg. Loss 0.602, Accuracy 79.1): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.66it/s]\n",
      "Epoch 21 train loss: 0.6017402954101563\n",
      "test_batch (Avg. Loss 0.923, Accuracy 69.9): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.53it/s]\n",
      "--- EPOCH 23/35 ---\n",
      "train_batch (Avg. Loss 0.567, Accuracy 80.4): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.72it/s]\n",
      "Epoch 22 train loss: 0.5666431884765625\n",
      "test_batch (Avg. Loss 0.919, Accuracy 70.3): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.30it/s]\n",
      "--- EPOCH 24/35 ---\n",
      "train_batch (Avg. Loss 0.533, Accuracy 81.6): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.66it/s]\n",
      "Epoch 23 train loss: 0.532683837890625\n",
      "test_batch (Avg. Loss 0.933, Accuracy 70.5): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.52it/s]\n",
      "--- EPOCH 25/35 ---\n",
      "train_batch (Avg. Loss 0.500, Accuracy 82.8): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.67it/s]\n",
      "Epoch 24 train loss: 0.4998658447265625\n",
      "test_batch (Avg. Loss 0.972, Accuracy 70.5): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.85it/s]\n",
      "--- EPOCH 26/35 ---\n",
      "train_batch (Avg. Loss 0.470, Accuracy 83.7): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.67it/s]\n",
      "Epoch 25 train loss: 0.47013522338867186\n",
      "test_batch (Avg. Loss 0.983, Accuracy 71.0): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.92it/s]\n",
      "--- EPOCH 27/35 ---\n",
      "train_batch (Avg. Loss 0.440, Accuracy 84.8): 100%|████████████████████████████████| 1000/1000 [00:35<00:00, 28.45it/s]\n",
      "Epoch 26 train loss: 0.440363037109375\n",
      "test_batch (Avg. Loss 1.092, Accuracy 69.5): 100%|██████████████████████████████████| 834/834 [00:08<00:00, 102.15it/s]\n",
      "--- EPOCH 28/35 ---\n",
      "train_batch (Avg. Loss 0.427, Accuracy 85.1): 100%|████████████████████████████████| 1000/1000 [00:36<00:00, 27.22it/s]\n",
      "Epoch 27 train loss: 0.42693936157226564\n",
      "test_batch (Avg. Loss 1.048, Accuracy 70.1): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.59it/s]\n",
      "--- EPOCH 29/35 ---\n",
      "train_batch (Avg. Loss 0.412, Accuracy 85.6): 100%|████████████████████████████████| 1000/1000 [00:35<00:00, 28.50it/s]\n",
      "Epoch 28 train loss: 0.41205908203125\n",
      "test_batch (Avg. Loss 1.079, Accuracy 70.0): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.46it/s]\n",
      "--- EPOCH 30/35 ---\n",
      "train_batch (Avg. Loss 0.399, Accuracy 86.0): 100%|████████████████████████████████| 1000/1000 [00:35<00:00, 28.56it/s]\n",
      "Epoch 29 train loss: 0.39899301147460936\n",
      "test_batch (Avg. Loss 0.995, Accuracy 72.0): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.61it/s]\n",
      "--- EPOCH 31/35 ---\n",
      "train_batch (Avg. Loss 0.371, Accuracy 86.7): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.59it/s]\n",
      "Epoch 30 train loss: 0.3711893310546875\n",
      "test_batch (Avg. Loss 0.976, Accuracy 73.2): 100%|██████████████████████████████████| 834/834 [00:08<00:00, 102.45it/s]\n",
      "--- EPOCH 32/35 ---\n",
      "train_batch (Avg. Loss 0.347, Accuracy 87.7): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.64it/s]\n",
      "Epoch 31 train loss: 0.3467991638183594\n",
      "test_batch (Avg. Loss 1.032, Accuracy 72.5): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.47it/s]\n",
      "--- EPOCH 33/35 ---\n",
      "train_batch (Avg. Loss 0.330, Accuracy 87.9): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.64it/s]\n",
      "Epoch 32 train loss: 0.3304111328125\n",
      "test_batch (Avg. Loss 1.067, Accuracy 72.0): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 106.53it/s]\n",
      "--- EPOCH 34/35 ---\n",
      "train_batch (Avg. Loss 0.303, Accuracy 89.0): 100%|████████████████████████████████| 1000/1000 [00:35<00:00, 28.57it/s]\n",
      "Epoch 33 train loss: 0.3034823303222656\n",
      "test_batch (Avg. Loss 1.127, Accuracy 71.8): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.92it/s]\n",
      "--- EPOCH 35/35 ---\n",
      "train_batch (Avg. Loss 0.280, Accuracy 89.7): 100%|████████████████████████████████| 1000/1000 [00:34<00:00, 28.61it/s]\n",
      "Epoch 34 train loss: 0.27975070190429685\n",
      "test_batch (Avg. Loss 1.135, Accuracy 72.7): 100%|██████████████████████████████████| 834/834 [00:07<00:00, 105.61it/s]\n",
      "*** Output file ./results\\test_run_L16_K64.json written\n"
     ]
    }
   ],
   "source": [
    "# TODO delete\n",
    "\n",
    "import hw2.experiments as experiments\n",
    "from hw2.experiments import load_experiment\n",
    "from cs236781.plot import plot_fit\n",
    "\n",
    "layers = [2, 4, 8, 16]\n",
    "\n",
    "for layers_per_block in layers:\n",
    "    experiments.run_experiment(\n",
    "        'test_run', seed=seed, bs_train=50, batches=20, epochs=35, early_stopping=5,\n",
    "        filters_per_layer=[32], layers_per_block=layers_per_block, pool_every=3, hidden_dims=[100],\n",
    "        model_type='resnet',\n",
    "    )\n",
    "\n",
    "    experiments.run_experiment(\n",
    "        'test_run', seed=seed, bs_train=50, batches=20, epochs=35, early_stopping=5,\n",
    "        filters_per_layer=[64], layers_per_block=layers_per_block, pool_every=3, hidden_dims=[100],\n",
    "        model_type='resnet',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1.1: Varying the network depth (`L`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll test the effect of the network depth on training.\n",
    "\n",
    "**Configuratons**:\n",
    "- `K=32` fixed, with `L=2,4,8,16` varying per run\n",
    "- `K=64` fixed, with `L=2,4,8,16` varying per run\n",
    "\n",
    "So 8 different runs in total.\n",
    "\n",
    "**Naming runs**:\n",
    "Each run should be named `exp1_1_L{}_K{}` where the braces are placeholders for the values. For example, the first run should be named `exp1_1_L2_K32`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Run the experiment on the above configuration with the `ConvClassifier` model. Make sure the result file names are as expected. Use the following blocks to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No results found for pattern exp1_1_L*_K32*.json.\n"
     ]
    }
   ],
   "source": [
    "plot_exp_results('exp1_1_L*_K32*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No results found for pattern exp1_1_L*_K64*.json.\n"
     ]
    }
   ],
   "source": [
    "plot_exp_results('exp1_1_L*_K64*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1.2: Varying the number of filters per layer (`K`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test the effect of the number of convolutional filters in each layer.\n",
    "\n",
    "**Configuratons**:\n",
    "- `L=2` fixed, with `K=[32],[64],[128],[256]` varying per run.\n",
    "- `L=4` fixed, with `K=[32],[64],[128],[256]` varying per run.\n",
    "- `L=8` fixed, with `K=[32],[64],[128],[256]` varying per run.\n",
    "\n",
    "So 12 different runs in total. To clarify, each run `K` takes the value of a list with a single element.\n",
    "\n",
    "**Naming runs**:\n",
    "Each run should be named `exp1_2_L{}_K{}` where the braces are placeholders for the values. For example, the first run should be named `exp1_2_L2_K32`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Run the experiment on the above configuration with the `ConvClassifier` model. Make sure the result file names are as expected. Use the following blocks to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No results found for pattern exp1_2_L2*.json.\n"
     ]
    }
   ],
   "source": [
    "plot_exp_results('exp1_2_L2*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No results found for pattern exp1_2_L4*.json.\n"
     ]
    }
   ],
   "source": [
    "plot_exp_results('exp1_2_L4*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No results found for pattern exp1_2_L8*.json.\n"
     ]
    }
   ],
   "source": [
    "plot_exp_results('exp1_2_L8*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1.3: Varying both the number of filters (`K`) and network depth (`L`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test the effect of the number of convolutional filters in each layer.\n",
    "\n",
    "**Configuratons**:\n",
    "- `K=[64, 128, 256]` fixed with `L=1,2,3,4` varying per run.\n",
    "\n",
    "So 4 different runs in total. To clarify, each run `K` takes the value of an array with a three elements.\n",
    "\n",
    "**Naming runs**:\n",
    "Each run should be named `exp1_3_L{}_K{}-{}-{}` where the braces are placeholders for the values. For example, the first run should be named `exp1_3_L1_K64-128-256`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Run the experiment on the above configuration  with the `ConvClassifier` model. Make sure the result file names are as expected. Use the following blocks to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No results found for pattern exp1_3*.json.\n"
     ]
    }
   ],
   "source": [
    "plot_exp_results('exp1_3*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1.4: Adding depth with Residual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test the effect of skip connections on the training and performance.\n",
    "\n",
    "**Configuratons**:\n",
    "- `K=[32]` fixed with `L=8,16,32` varying per run.\n",
    "- `K=[64, 128, 256]` fixed with `L=2,4,8` varying per run.\n",
    "\n",
    "So 6 different runs in total.\n",
    "\n",
    "**Naming runs**:\n",
    "Each run should be named `exp1_4_L{}_K{}-{}-{}` where the braces are placeholders for the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Run the experiment on the above configuration with the `ResNetClassifier` model. Make sure the result file names are as expected. Use the following blocks to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No results found for pattern exp1_4_L*_K32.json.\n"
     ]
    }
   ],
   "source": [
    "plot_exp_results('exp1_4_L*_K32.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No results found for pattern exp1_4_L*_K64*.json.\n"
     ]
    }
   ],
   "source": [
    "plot_exp_results('exp1_4_L*_K64*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Custom network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you will create your own custom network architecture based on the `ConvClassifier` you've implemented.\n",
    "\n",
    "Try to overcome some of the limitations your experiment 1 results, using what you learned in the course.\n",
    "\n",
    "You are free to add whatever you like to the model, for instance \n",
    "- Batch normalization\n",
    "- Dropout layers\n",
    "- Skip connections\n",
    "- Change kernel spatial sizes and strides\n",
    "- Custom blocks or ideas from known architectures (e.g. inception module)\n",
    "\n",
    "Just make sure to keep the model's `init` API identical (or maybe just add parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement your custom architecture in the `YourCodeNet` class within the `hw2/cnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YourCodeNet(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=20000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "out = tensor([[-0.4733, -0.5248,  0.1698,  0.2607,  0.9842, -0.1944,  0.7836,  0.6179,\n",
      "          0.8109, -0.2685]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = cnn.YourCodeNet((3,100,100), 10, channels=[32]*4, pool_every=2, hidden_dims=[100]*2)\n",
    "print(net)\n",
    "\n",
    "test_image = torch.randint(low=0, high=256, size=(3, 100, 100), dtype=torch.float).unsqueeze(0)\n",
    "test_out = net(test_image)\n",
    "print('out =', test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2 Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your custom model on at least the following:\n",
    "\n",
    "**Configuratons**:\n",
    "- `K=[32, 64, 128]` fixed with `L=3,6,9,12` varying per run.\n",
    "\n",
    "So 4 different runs in total. To clarify, each run `K` takes the value of an array with a three elements.\n",
    "\n",
    "If you want, you can add some extra runs following the same pattern.\n",
    "Try to see how deep a model you can train.\n",
    "\n",
    "**Naming runs**:\n",
    "Each run should be named `exp2_L{}_K{}-{}-{}-{}` where the braces are placeholders for the values. For example, the first run should be named `exp2_L3_K32-64-128`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Run the experiment on the above configuration with the `YourCodeNet` model. Make sure the result file names are as expected. Use the following blocks to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No results found for pattern exp2*.json.\n"
     ]
    }
   ],
   "source": [
    "plot_exp_results('exp2*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "<a id=part3_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw2/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw2.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "Analyze your results from experiment 1.1. In particular,\n",
    "1.  Explain the effect of depth on the accuracy. What depth produces the best results and why do you think that's the case?\n",
    "1. Were there values of `L` for which the network wasn't trainable? what causes this? Suggest two things which may be done to resolve it at least partially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Your answer:**\n",
       "\n",
       "\n",
       "Write your answer using **markdown** and $\\LaTeX$:\n",
       "```python\n",
       "# A code block\n",
       "a = 2\n",
       "```\n",
       "An equation: $e^{i\\pi} -1 = 0$\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw2.answers.part3_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "\n",
    "Analyze your results from experiment 1.2. In particular, compare to the results of experiment 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Your answer:**\n",
       "\n",
       "\n",
       "Write your answer using **markdown** and $\\LaTeX$:\n",
       "```python\n",
       "# A code block\n",
       "a = 2\n",
       "```\n",
       "An equation: $e^{i\\pi} -1 = 0$\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw2.answers.part3_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 \n",
    "\n",
    "Analyze your results from experiment 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Your answer:**\n",
       "\n",
       "\n",
       "Write your answer using **markdown** and $\\LaTeX$:\n",
       "```python\n",
       "# A code block\n",
       "a = 2\n",
       "```\n",
       "An equation: $e^{i\\pi} -1 = 0$\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw2.answers.part3_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Analyze your results from experiment 1.4. Compare to experiment 1.1 and 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Your answer:**\n",
       "\n",
       "\n",
       "Write your answer using **markdown** and $\\LaTeX$:\n",
       "```python\n",
       "# A code block\n",
       "a = 2\n",
       "```\n",
       "An equation: $e^{i\\pi} -1 = 0$\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw2.answers.part3_q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "1. Explain your modifications to the architecture which you implemented in the `YourCodeNet` class.\n",
    "2. Analyze the results of experiment 2. Compare to experiment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Your answer:**\n",
       "\n",
       "\n",
       "Write your answer using **markdown** and $\\LaTeX$:\n",
       "```python\n",
       "# A code block\n",
       "a = 2\n",
       "```\n",
       "An equation: $e^{i\\pi} -1 = 0$\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw2.answers.part3_q5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
